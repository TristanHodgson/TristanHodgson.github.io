<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <link rel="icon" type="image/svg+xml" href="img/favicon.svg" />
        <link rel="stylesheet" href="style.css" />
        <title>Tristan Hodgson</title>
    </head>
    <body>
        <section class="mid">
            <h1>Tristan Hodgson</h1>
            <p class="byline">CV available upon request</p>
            <br />
            <div class="icons">
                <a class="icon_link" href="https://github.com/TristanHodgson">
                    <img
                        class="icon"
                        src="./img/github.svg"
                        alt="Github logo"
                    />
                    <span class="description">Github</span>
                </a>
                <a class="icon_link" href="mailto:contact@tristanhodgson.com">
                    <img
                        class="icon"
                        src="./img/email.svg"
                        alt="Envelope icon representing email"
                    />
                    <span class="description">contact@tristanhodgson.com</span>
                </a>
            </div>
            <a href="./index.html"><div class="callout">Home</div></a>
            <dev class="article">
                <h2 id="LLMs_2025-07-30">What's the Deal with LLMs?</h2>
                <small>2025-07-30</small>
                <p>Recently I was talking about Large Language Models (LLMs) with the CEO of a large UK based healthcare charity, we were discussing the frequently ignored social issues that accompany these tools that have become quite ubiquitous in our everyday lives. What I discovered, during the course of this conversation, was that, while I do quite frequently think about these issues, I rarely think of the ways in which these issues are interconnected. I also recently read a popular article from <a href="https://situational-awareness.ai/">SITUATIONAL AWARENESS: The Decade Ahead by Leopold Aschenbrenner</a> -- a former employee of OpenAI -- that was very optimistic and in my opinion flawed in it's approach to the issues. This has therefore prompted me to write this article as a snapshot of my current thoughts on the issues that the field of LLMs face both now and into the future.</p>
                <p>Before I go any further, however, there are a few things that I think need addressing. Firstly is that I will almost certainly change my mind and be wrong about most of the issues that I outline, however I do not view this as an issue, after all changing ones mind is an important part of scientific thinking, especially in such a fast moving industry. Morover, I've structured this article as a kind of response to the article by Aschenbrenner, pointing out where I agree and disagree with the current state ofMLas it has provided a useful framework from which to build my research and writing (I am after all a non-expert from outside the industry).</p>
                <h3>Investment</h3>
                <p>It's no secret that labs that work on Machine Learning (ML) have received a lot of investment. Companies are already spending on the order of <a href="https://www.cnbc.com/2025/02/08/tech-megacaps-to-spend-more-than-300-billion-in-2025-to-win-in-ai.html">100s of billions</a> each year AI, and this investment will only continue to grow with <a href="https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/the-cost-of-compute-a-7-trillion-dollar-race-to-scale-data-centers">Mckinsey predicting this cumulating to $4.4 trillion by 2030.</a> It has been suggested that current models cost on the order of <a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/ai-models-that-cost-dollar1-billion-to-train-are-in-development-dollar100-billion-models-coming-soon-largest-current-models-take-only-dollar100-million-to-train-anthropic-ceo">$100 million to train and yet within three years could cost as much as $100 billion.</a> Yet in spite of the enormous investment and interest in the sector, profits are often <a href="https://www.cnbc.com/2025/06/09/openai-hits-10-billion-in-annualized-revenue-fueled-by-chatgpt-growth.html">non-existant</a>.</p>
                <p>I believe that investing in science and technology produces good outcomes for us all (e.g. <a href="https://www.jpl.nasa.gov/infographics/20-inventions-we-wouldnt-have-without-space-travel/">NASA</a>, <a href="https://home.cern/about/key-achievements">CERN</a>) and there's can be no doubt that LLMs are technologically impressive. Yet, in spite of this, it seems obvious to me that the level of investment currently going on will not be sustained unless LLM labs can figure out a way to be profitable.</p>
                <img src="img/LLMs_2025-07-30/1.png" alt="The training compute of notable LLMs has been doubling roughly every five months" width="100%">
                <figcaption>Source: <a href="https://epoch.ai/data/ai-models#handorgel1-fold1-header">Epoch AI Data Repository</a></figcaption>
                <p>Aschenbrenner is very optimistic about the idea that LLMs will increase in power as it has done in the past--that this trend line will remain unbroken. In the article, he outlines the three main reasons for this growth in capability: increases in compute, new innovative algorithms, and finding new ways of using existing models (e.g. agentic modes and reasoning models). Nevertheless it appears to me that all three of these factors depend heavily on continued investment. As we saw earlier compute is very expensive and newer models are likely to demand more of it; would the same gains be possible if <a href="https://doi.org/10.1609/aaai.v38i18.29959">compute power was more limited</a>? Similar issues emerge related to talent. Currently, LLM labs pay very high <a href="https://www.businessinsider.com/top-ai-startup-companies-salaries-pay-data-openai-anthropic-perplexity-2025-7?op=1#abridge-1">salaries</a>, and are willing to acquire the best and brightest by any means necessary. Having highly intelligent researchers means that the issues associated withMLresearch get easier to solve. Therefore, it seems plausible to me that if investment were to decrease and salaries went down, progress would slow as talent decreased. To quote every investment advert ever: past performance is not an indication of future results.</p>
                <p>The issue that has captured public imagination the most is the issue of data scarcity, the idea that as companies have already scrapped most of the web in search of valuable training data (I'll come onto the issues with this practice later) there is a risk that they will <a href="https://arxiv.org/pdf/2305.17493.pdf">run out</a>, creating a brick wall for progress. I'm less unconvinced by this argument, Aschenbrenner many potential approaches which are being explored. At time of writing, no solutions have been announced (AI labs often work in secret to maintain technological supremacy) though I predict that this problem will not prove insurmountable.</p>
                <h3>Performance</h3>
                <p>Given that we now understand why investment is such a big issue for these research labs, how do they plan to deal with this? Their <a href="https://www.nytimes.com/2024/09/27/technology/openai-chatgpt-investors-funding.html">plan</a> seems to be that of a startup: burn through lots of money in the beginning by attracting lots of users, then begin to extract more and more money from these customers as time goes on until profitability is achieved.</p>
                <p>Critically, this relies on individuals and, more importantly, companies seeing value in the services that LLMs provide. I think that these research labs make a critical mistake in this regard, they assume that if they make better and better models, the users will follow. Aschenbrenner claims that current models are similar in competence to a PhD student outside of their own discipline, yet <a href="https://www.hiringlab.org/uk/blog/2024/08/29/job-postings-formal-education-requirements/">very few jobs require a PhD</a> so by this logic most jobs that can be replaced already should've been. This line of thinking shows that perhaps LLMs are already good enough at the reasoning tasks on which we benchmark them and it is instead other factors that prevent their adoption.</p>
                <p>The first and perhaps most simple issue is that we are optimising for the wrong thing. Labs are spending enormous quantities of time and effort improving their scores at many <a href="https://epoch.ai/benchmarks">benchmarks</a> most of which revolve around complex <a href="https://epoch.ai/frontiermath">mathematical</a> and other <a href="https://arxiv.org/abs/2311.12022">very complex academic tasks</a>. Despite this, most people use LLMs for very different tasks some the largest <a href="https://www.forbes.com/councils/forbestechcouncil/2024/03/07/successful-real-world-use-cases-for-llms-and-lessons-they-teach/">use cases</a> for LLMs include summarisation of text and chat-bots imbedded into web pages. It seems to me that at least part of the reason that software developers have such a <a href="https://survey.stackoverflow.co/2024/ai">high uptake</a> in LLM usage is partially due to labs optimising so heavily for <a href="https://openai.com/index/introducing-swe-bench-verified/">software devlopment</a>. To some extent this is understandable: most ML researchers come with some level of mathematical or academic background so they are more comfortable building these benchmarks, combined with the fact that quantitative benchmarks are much easier to score than qualitative ones. Some attempts to correct this do <a href="https://doi.org/10.48550/arXiv.2503.05244">exist</a>, but it seems clear that we are a long way off from <a href="https://on.ft.com/45eaulZ">compelling LLM authored novels</a>. This matters because LLMs write slightly <a href="https://www.scientificamerican.com/article/chatgpt-is-changing-the-words-we-use-in-conversation/">unusual text</a>, as you can probably tell, I love an <a href="https://medium.com/@brentcsutoras/the-em-dash-dilemma-how-a-punctuation-mark-became-ais-stubborn-signature-684fbcc9f559">em-dash</a>, but not to the same extent as Chat GPT.</p>
                <p>Another issue is simply one of inertia. Transitions take time, to suggest that LLMs adaption will be overnight is foolish, it will be a slow transition where business leaders need to figure out how best to integrate this new technology. This transition will be difficult and mistake will be made. Regretfully, many executives, seem to feel a need to get LLMs to everything as quickly as possible <a href="https://faq.whatsapp.com/1002544104126998">regardless of its utility to the user</a>. It seems to me that some need to return to the ideas of user centric design -- starting with a problem before presenting a solution rather than starting with a technology and creating a solution to a problem that may not exist. I somewhat sympathise with <a href="https://www.independent.co.uk/tech/apple-ai-artificial-intelligence-iphone-b2769653.html">Apple</a> in this regard; in the <a href="https://www.linkedin.com/pulse/why-apple-avoided-term-ai-years-theyre-embracing-now-alex-choong-y0loc">past</a> Apple always avoided talking about the ML tech that made their products work, yet recently, investor backlash has forced them to discus their use ML tech in an effort to appear up to date. WWDC 2024 was arguable the most significant moment for the perception of ML in business for precisely this reason.</p>
                <p>Furthermore, there are issues of accountability. The <a href="https://www.ibm.com/think/insights/ai-decision-making-where-do-businesses-draw-the-line">IBM Training Manual 1979</a> contains a quote which is as pertinent today as the day it was written: "A computer can never be held accountable, therefore a computer must never make a management decision". How do we decide responsibility when a ML system <a href="https://www.theguardian.com/technology/2024/oct/23/character-ai-chatbot-sewell-setzer-death">kills somebody</a> or makes <a href="https://www.politico.com/news/magazine/2025/07/10/musk-grok-hitler-ai-00447055">horrendous, racist comments</a> (remember that similar systems are already making important decisions around topics like <a href="https://www.ft.com/content/87c1bf22-9ad6-4d6a-94e2-270c8633dc8a">hiring</a>). I'll be honest, this is a problem that worries me greatly; we were all so worried about AI machines taking over the world and starting to kill people that we missed it when it began.</p>
                <p>LLMs are fundamentally stochastic in nature so as we give them more to do, the number of mistakes they make will increase by the law or large numbers. While there are people working on these issues of <a href="https://www.livescience.com/technology/artificial-intelligence/ai-hallucinates-more-frequently-as-it-gets-more-advanced-is-there-any-way-to-stop-it-from-happening-and-should-we-even-try">hallucination</a> and implementing <a href="https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-are-ai-guardrails">guardrails</a>, the inherently probabilistic nature of these systems means that I don't hold out much hope, after all, <a href="https://www.youtube.com/watch?v=leX541Dr2rU">there is no algorithm for truth</a>.</p>
                <h3>ESG Issues</h3>
                <p>The ESG issues associated with ML are plentiful, so plentiful, in fact, that there is no possible way that I can cover them all in the depth I would like without this article ballooning in size even more than it already has. What I will do instead is briefly outline a few of the issues for which I feel I have an interesting point to make.</p>
                <p><b>Copyright</b>&emsp; At this point, it is clear that LLMs are built on the back of <a href="https://arstechnica.com/features/2025/06/study-metas-llama-3-1-can-recall-42-percent-of-the-first-harry-potter-book/">stolen works</a>. It also seems as though courts will <a href="https://www.theguardian.com/technology/2025/jun/25/anthropic-did-not-breach-copyright-when-training-ai-on-books-without-permission-court-rules">uphold</a> their conduct in an attempt to avoid stifling innovation (though cases are still <a href="https://www.reuters.com/legal/litigation/judge-explains-order-new-york-times-openai-copyright-case-2025-04-04/">playing out</a>). My controversial opinion on this is that the cat is somewhat out of the bag; we want to live in a world where copyright means something. However, even if we rule that domestic companies cannot steal, the models built outside of our jurisdiction will not follow our laws, hence we would be left behind. My tentative solution to this would be a long overdue reform of copyright law. I'm largely convinced by <a href="https://www.youtube.com/watch?v=1Jwo5qc78QU">Tom Scott's</a> argument for copyright lasting for 20 years. This would allow large quantities of the internet to be scrapped, but new data would be protected. This would encourage deals with news publishers and product reviewers this would allow us to back track slightly on the <a href="https://housefresh.com/beware-of-the-google-ai-salesman/">great decoupling</a> such that there is still and open web left in a world of zero-click searches.</p>
                <p><b>Jobs</b>&emsp; If the implementation of LLMs is successful then some <a href="https://www.youtube.com/watch?v=7Pq-S557XQU">jobs</a> will disappear. In fact, we are already seeing some <a href="https://on.ft.com/3IKa5jE">evidence</a> of the beginning of this. The argument that people make is that new jobs will be created, I remain unconvinced by this argument. Many of the new jobs associated with AI that I have seen so far have been low-skilled data-labeling <a href="https://www.youtube.com/watch?v=AaU6tI2pb3M">jobs</a>. That being said I could be wrong about this, there was a recent <a href="https://on.ft.com/4kOlnAO">article</a> about replacing high-skilled data labeling, this is perhaps marginally better.</p>
                <p><b>Climate and the Environment</b>&emsp; I don't much to add on this point that hasn't already been said. I particularly like this <a href="https://www.youtube.com/watch?v=5sFBySzNIX0">video</a> by Dr. Simon Clark as it very well researched and strikes a good balance of avoiding alarmism and acknowledging the scale of the issue (including how little we know about the scale of the issue).</p>
                <p><b>Consolidation and Privacy</b>&emsp;The resources required to run the state of the art LLMs are enormous, in a recent <a href="https://www.youtube.com/watch?v=T17bpGItqXw">video</a> about running a Quantised version of Deepseek R1, the stated machine has 128GB of memory which alone costs close to £300. This is unachievable to nearly everyone, this means that, most people have to rely on the large cloud based solutions. Yet, we have already seen these companies <a href="https://www.malwarebytes.com/blog/news/2025/07/">abuse</a> this position. The simplest solution to the data scarcity problem is to collect more data, the easiest way to collect this new data is to mine it from your users. I predict that we will see more schemes like those perpetrated by Google over the coming years.</p>

                <p><b>Alignment</b>&emsp; Perhaps the issue that people give the most stock to is alignment. Alignment as an issue comes in two forms, superintelligence and everyday. While there are some very scary sounding papers about LLMs <a href="https://doi.org/10.1073/pnas.2317967121">deceiving</a> people, I'm not wholly convinced by the argument of <a href="https://medium.com/the-modern-scientist/the-biggest-lie-about-language-models-ai-emergence-b2fe807de5ac">emergence</a>. That being said, I'm far from an expert so could very well be wrong. What interests me more is the way in which current, commercial LLMs display mis-alignment. Be it the recent controversy where Grok <a href="https://www.bbc.co.uk/news/articles/c4g8r34nxeno">"praised Hitler"</a> or <a href="https://spectrum.ieee.org/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation">Microsoft Tay</a> becoming very antisocial, we have seen many times that LLMs are not immune from <a href="https://www.theguardian.com/technology/2024/mar/16/ai-racism-chatgpt-gemini-bias">noticing and exasperating negative patterns in society</a>. This will only become more and more of an issue as we let LLMs control more and more of our lives, remember they already influence hiring and related ML algorithms are already used in making <a href="https://royalsociety.org/medals-and-prizes/science-book-prize/books/2018/hello-world/">legal decsions</a>. It appears that we have descided as a society to implement first and implement checks and balances second, people will be harmed by this choice.</p>
                <h3>Benefits and how I use it</h3>
                <h3>Conclusion</h3>
                To attempt to sum this all up, I don't dislike LLMs; I think they are a tremendous technical achievment. I do however think that we need to stop and think about our expectations for these systems because the reality of the situation is that they are not the one simple trick to saving the world as so many pretend that they are. There are serious issues to solve, both technical and philosophically before we should trust serious tasks to LLMs, we need to decide the world we want to live in and I worry about our trajectory.
            </dev>
        </section>
    </body>
</html>

            </dev>
        </section>
    </body>
</html>
